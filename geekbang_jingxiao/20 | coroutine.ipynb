{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 协程是实现并发编程的一种方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NGINX 事件循环\n",
    "事件循环启动一个统一的调度器，让调度器来决定一个时刻去运行哪个任务，于是省却了多线程中启动线程、管理线程、同步锁等各种开销。同一时期的 NGINX，在高并发下能保持低资源低消耗高性能，相比 Apache 也支持更多的并发连接。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从一个爬虫说起\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "def crawl_page(url):\n",
    "    print('crawling {}'.format(url))\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    time.sleep(sleep_time)\n",
    "    print('OK {}'.format(url))\n",
    "\n",
    "def main(urls):\n",
    "    for url in urls:\n",
    "        crawl_page(url)\n",
    "\n",
    "%time main(['url_1', 'url_2', 'url_3', 'url_4'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile coroutine_1.py\n",
    "\n",
    "import asyncio\n",
    "import time\n",
    "def my_profile(func):\n",
    "    def wrapper(url):\n",
    "        starttime = time.time()\n",
    "        func(url)\n",
    "        endtime = time.time()\n",
    "        du = endtime -starttime\n",
    "        return du        \n",
    "    return wrapper\n",
    "\n",
    "async def crawl_page(url):\n",
    "    print('crawling {}'.format(url))\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    await asyncio.sleep(sleep_time)\n",
    "    print('OK {}'.format(url))\n",
    "    \n",
    "# @my_profile\n",
    "async def main(urls):\n",
    "    for url in urls:\n",
    "        await crawl_page(url)\n",
    "        \n",
    "starttime = time.time()\n",
    "asyncio.run(main(['url_1', 'url_2', 'url_3', 'url_4']))\n",
    "endtime = time.time()\n",
    "du = endtime -starttime\n",
    "print('du is {}'.format(du))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python coroutine_1.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- async 修饰词声明异步函数，于是，这里的 crawl_page 和 main 都变成了异步函数。而调用异步函数，我们便可得到一个协程对象（coroutine object）。\n",
    "## 执行协程的多种方法\n",
    "- 介绍三种\n",
    "    - await\n",
    "    await 执行的效果，和 Python 正常执行是一样的，也就是说程序会阻塞在这里，进入被调用的协程函数，执行完毕返回后再继续\n",
    "    - asyncio.create_task() 来创建任务\n",
    "    - asyncio.run 来触发运行 Python 3.7 之后才有的特性，可以让 Python 的协程接口变得非常简单 不用去理会事件循环怎么定义和怎么使用的问题\n",
    "        一个非常好的编程规范是，asyncio.run(main()) 作为主程序的入口函数，在程序运行周期内，只调用一次 asyncio.run。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务（Task）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile coroutine_task_1.py\n",
    "\n",
    "import time\n",
    "import asyncio\n",
    "\n",
    "async def crawl_page(url):\n",
    "    print('crawling {}'.format(url))\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    await asyncio.sleep(sleep_time)\n",
    "    print('OK {}'.format(url))\n",
    "\n",
    "async def main(urls):\n",
    "    tasks = [asyncio.create_task(crawl_page(url)) for url in urls]\n",
    "    for task in tasks:\n",
    "        await task\n",
    "\n",
    "starttime = time.time()\n",
    "asyncio.run(main(['url_1', 'url_2', 'url_3', 'url_4']))\n",
    "endtime = time.time()\n",
    "du = endtime -starttime\n",
    "print('du is {}'.format(du))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python coroutine_task_1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile coroutine_task_2.py\n",
    "\n",
    "import asyncio\n",
    "import time\n",
    "async def crawl_page(url):\n",
    "    print('crawling {}'.format(url))\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    await asyncio.sleep(sleep_time)\n",
    "    print('OK {}'.format(url))\n",
    "\n",
    "async def main(urls):\n",
    "    tasks = [asyncio.create_task(crawl_page(url)) for url in urls]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "starttime = time.time()\n",
    "asyncio.run(main(['url_1', 'url_2', 'url_3', 'url_4']))\n",
    "endtime = time.time()\n",
    "du = endtime -starttime\n",
    "print('du is {}'.format(du))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *tasks 解包列表，将列表变成了函数的参数；与之对应的是， ** dict 将字典变成了函数的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python coroutine_task_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile coroutine_task_3.py\n",
    "\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "async def worker_1():\n",
    "    print('worker_1 start')\n",
    "    await asyncio.sleep(3)\n",
    "    print('worker_1 done')\n",
    "\n",
    "async def worker_2():\n",
    "    print('worker_2 start')\n",
    "    await asyncio.sleep(2)\n",
    "    print('worker_2 done')\n",
    "\n",
    "async def main():\n",
    "    task1 = asyncio.create_task(worker_1())\n",
    "    task2 = asyncio.create_task(worker_2())\n",
    "    print('before await')\n",
    "    await task1\n",
    "    print('awaited worker_1')\n",
    "    await task2\n",
    "    print('awaited worker_2')\n",
    "starttime = time.time()\n",
    "asyncio.run(main())\n",
    "endtime = time.time()\n",
    "du = endtime -starttime\n",
    "print('du is {}'.format(du))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python coroutine_task_3.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 协程任务限定运行时间\n",
    "### 协程运行时出现错误处理\n",
    "- return_exceptions=True 不会throw到执行层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile coroutine_task_4.py\n",
    "\n",
    "import asyncio\n",
    "\n",
    "async def worker_1():\n",
    "    await asyncio.sleep(1)\n",
    "    return 1\n",
    "\n",
    "async def worker_2():\n",
    "    await asyncio.sleep(2)\n",
    "    return 2 / 0\n",
    "\n",
    "async def worker_3():\n",
    "    await asyncio.sleep(3)\n",
    "    return 3\n",
    "\n",
    "async def main():\n",
    "    task_1 = asyncio.create_task(worker_1())\n",
    "    task_2 = asyncio.create_task(worker_2())\n",
    "    task_3 = asyncio.create_task(worker_3())\n",
    "\n",
    "    await asyncio.sleep(2)\n",
    "    task_3.cancel()\n",
    "\n",
    "    res = await asyncio.gather(task_1, task_2, task_3, return_exceptions=True)\n",
    "    print(res)\n",
    "\n",
    "asyncio.run(main())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python coroutine_task_4.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生产者和消费者模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile coroutine_producer_customer_model.py\n",
    "\n",
    "import asyncio\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "async def consumer(queue, id):\n",
    "    while True:\n",
    "        val = await queue.get()\n",
    "        print('{} get a val: {}'.format(id, val))\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "async def producer(queue, id):\n",
    "    for i in range(5):\n",
    "        val = random.randint(1, 10)\n",
    "        await queue.put(val)\n",
    "        print('{} put a val: {}'.format(id, val))\n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "async def main():\n",
    "    queue = asyncio.Queue()\n",
    "\n",
    "    consumer_1 = asyncio.create_task(consumer(queue, 'consumer_1'))\n",
    "    consumer_2 = asyncio.create_task(consumer(queue, 'consumer_2'))\n",
    "\n",
    "    producer_1 = asyncio.create_task(producer(queue, 'producer_1'))\n",
    "    producer_2 = asyncio.create_task(producer(queue, 'producer_2'))\n",
    "\n",
    "    await asyncio.sleep(10)\n",
    "    consumer_1.cancel()\n",
    "    consumer_2.cancel()\n",
    "    \n",
    "    await asyncio.gather(consumer_1, consumer_2, producer_1, producer_2, return_exceptions=True)\n",
    "\n",
    "\n",
    "s = time.time()\n",
    "asyncio.run(main())\n",
    "e = time.time()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python coroutine_producer_customer_model.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实战：豆瓣近日推荐电影爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "阿凡达：水之道 12月16日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2884182275.jpg\n",
      "谁偷了我的粉兔子 12月16日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2884307337.jpg\n",
      "穿靴子的猫2 12月23日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2885032978.jpg\n",
      "绑架游戏 12月23日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2869898539.jpg\n",
      "唬胆特工 12月23日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2885052777.jpg\n",
      "繁华将至 12月23日 https://img2.doubanio.com/view/photo/s_ratio_poster/public/p2628561183.jpg\n",
      "龙马！新生网球王子 12月23日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2884562826.jpg\n",
      "龙马精神 12月31日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2883750366.jpg\n",
      "保你平安 12月31日 https://img2.doubanio.com/view/photo/s_ratio_poster/public/p2885144191.jpg\n",
      "透明侠侣 12月31日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2876027489.jpg\n",
      "绝望主夫 12月31日 https://img2.doubanio.com/view/photo/s_ratio_poster/public/p2884933443.jpg\n",
      "女生规则 01月06日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2885065194.jpg\n",
      "龙珠超：超级人造人 01月06日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2885127296.jpg\n",
      "极速保镖 01月06日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2883525799.jpg\n",
      "流浪地球2 01月22日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2884201204.jpg\n",
      "熊出没·伴我“熊芯” 01月22日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2884230767.jpg\n",
      "CPU times: user 1.24 s, sys: 32.7 ms, total: 1.27 s\n",
      "Wall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def main():\n",
    "    url = \"https://movie.douban.com/cinema/later/beijing/\"\n",
    "    head={\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.113 Safari/537.36',\n",
    "    'Referer':'https://time.geekbang.org/column/article/101855',\n",
    "    'Connection':'keep-alive'}\n",
    "    init_page = requests.get(url,headers=head).content\n",
    "    # init_page = requests.get(url).content\n",
    "    # init_soup = BeautifulSoup(init_page, 'lxml')\n",
    "    init_soup = BeautifulSoup(init_page, 'html.parser')\n",
    "\n",
    "    all_movies = init_soup.find('div', id=\"showing-soon\")\n",
    "    for each_movie in all_movies.find_all('div', class_=\"item\"):\n",
    "        all_a_tag = each_movie.find_all('a')\n",
    "        all_li_tag = each_movie.find_all('li')\n",
    "\n",
    "        movie_name = all_a_tag[1].text\n",
    "        url_to_fetch = all_a_tag[1]['href']\n",
    "        movie_date = all_li_tag[0].text\n",
    "\n",
    "        response_item = requests.get(url_to_fetch,headers=head).content\n",
    "        # soup_item = BeautifulSoup(response_item, 'lxml')\n",
    "        soup_item = BeautifulSoup(response_item, 'html.parser')\n",
    "        img_tag = soup_item.find('img')\n",
    "\n",
    "        print('{} {} {}'.format(movie_name, movie_date, img_tag['src']))\n",
    "\n",
    "%time main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting coroutine_crawl.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile coroutine_crawl.py\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "header={\n",
    "'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.113 Safari/537.36',\n",
    "'Referer':'https://time.geekbang.org/column/article/101855',\n",
    "'Connection':'keep-alive'}\n",
    "async def fetch_content(url):\n",
    "    async with aiohttp.ClientSession(\n",
    "        headers=header, connector=aiohttp.TCPConnector(ssl=False)\n",
    "    ) as session:\n",
    "        async with session.get(url) as response:\n",
    "            return await response.text()\n",
    "\n",
    "async def main():\n",
    "    url = \"https://movie.douban.com/cinema/later/beijing/\"\n",
    "\n",
    "    init_page = await fetch_content(url)\n",
    "    init_soup = BeautifulSoup(init_page, 'html.parser')\n",
    "\n",
    "    movie_names, urls_to_fetch, movie_dates = [], [], []\n",
    "\n",
    "    all_movies = init_soup.find('div', id=\"showing-soon\")\n",
    "    for each_movie in all_movies.find_all('div', class_=\"item\"):\n",
    "        all_a_tag = each_movie.find_all('a')\n",
    "        all_li_tag = each_movie.find_all('li')\n",
    "\n",
    "        movie_names.append(all_a_tag[1].text)\n",
    "        urls_to_fetch.append(all_a_tag[1]['href'])\n",
    "        movie_dates.append(all_li_tag[0].text)\n",
    "\n",
    "    tasks = [fetch_content(url) for url in urls_to_fetch]\n",
    "    pages = await asyncio.gather(*tasks)\n",
    "\n",
    "    for movie_name, movie_date, page in zip(movie_names, movie_dates, pages):\n",
    "        soup_item = BeautifulSoup(page, 'html.parser')\n",
    "        img_tag = soup_item.find('img')\n",
    "\n",
    "        print('{} {} {}'.format(movie_name, movie_date, img_tag['src']))\n",
    "\n",
    "s = time.time()\n",
    "asyncio.run(main())\n",
    "e= time.time()\n",
    "print(e-s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2136.22s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "阿凡达：水之道 12月16日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2884182275.jpg\n",
      "谁偷了我的粉兔子 12月16日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2884307337.jpg\n",
      "穿靴子的猫2 12月23日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2885032978.jpg\n",
      "绑架游戏 12月23日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2869898539.jpg\n",
      "唬胆特工 12月23日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2885052777.jpg\n",
      "繁华将至 12月23日 https://img2.doubanio.com/view/photo/s_ratio_poster/public/p2628561183.jpg\n",
      "龙马！新生网球王子 12月23日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2884562826.jpg\n",
      "龙马精神 12月31日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2883750366.jpg\n",
      "保你平安 12月31日 https://img2.doubanio.com/view/photo/s_ratio_poster/public/p2885144191.jpg\n",
      "透明侠侣 12月31日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2876027489.jpg\n",
      "绝望主夫 12月31日 https://img2.doubanio.com/view/photo/s_ratio_poster/public/p2884933443.jpg\n",
      "女生规则 01月06日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2885065194.jpg\n",
      "龙珠超：超级人造人 01月06日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2885127296.jpg\n",
      "极速保镖 01月06日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2883525799.jpg\n",
      "流浪地球2 01月22日 https://img9.doubanio.com/view/photo/s_ratio_poster/public/p2884201204.jpg\n",
      "熊出没·伴我“熊芯” 01月22日 https://img1.doubanio.com/view/photo/s_ratio_poster/public/p2884230767.jpg\n",
      "2.0485482215881348\n"
     ]
    }
   ],
   "source": [
    "!python coroutine_crawl.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 协程如何实现回调函数\n",
    "- task 对象调用 add_done_callback() 函数，即可绑定特定回调函数。回调函数接受一个 future 对象，可以通过 future.result() 来获取协程函数的返回值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting coroutine_callback.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile coroutine_callback.py\n",
    "import asyncio\n",
    "import time\n",
    "async def crawl_page(url):\n",
    "    print('crawling {}'.format(url))\n",
    "    sleep_time = int(url.split('_')[-1])\n",
    "    await asyncio.sleep(sleep_time)\n",
    "    return 'OK {}'.format(url)\n",
    "\n",
    "async def main(urls):\n",
    "    tasks = [asyncio.create_task(crawl_page(url)) for url in urls]\n",
    "    for task in tasks:\n",
    "        task.add_done_callback(lambda future: print('result: ', future.result()))\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "s = time.time()\n",
    "asyncio.run(main(['url_1', 'url_2', 'url_3', 'url_4']))\n",
    "e = time.time()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2585.09s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawling url_1\n",
      "crawling url_2\n",
      "crawling url_3\n",
      "crawling url_4\n",
      "result:  OK url_1\n",
      "result:  OK url_2\n",
      "result:  OK url_3\n",
      "result:  OK url_4\n",
      "4.00203800201416\n"
     ]
    }
   ],
   "source": [
    "!python coroutine_callback.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mltools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce1cd4a375fc1416293ca7d886fb469d5d778c783bfa8fb4db8e9db71b828d70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
